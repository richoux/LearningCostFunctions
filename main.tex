\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai20.sty is NOT the same than previous years'
\usepackage{ijcai20}

\usepackage[english]{babel}
\usepackage[T1]{fontenc}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}

\usepackage{xspace}
\usepackage[dvipsnames, table]{xcolor}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{adjustbox}
%\usepackage[ruled,linesnumbered]{algorithm2e}

\title{Automatic Cost Function Generation ('learning' in the title?)}

\iffalse
\author{
Florian Richoux$^{1,2}$
\and
Jean-François Baffier$^3$
\affiliations
$^1$JFLI, CNRS, NII\\
$^2$Université de Nantes\\
$^3$RIKEN AIP
\emails
florian.richoux@polytechnique.edu,
jf@baffier.fr
}
\fi

\newcommand{\ie}{\textit{i.e.}}
\newcommand{\cp}{\textsc{CP}\xspace}
\newcommand{\csp}{\textsc{CSP}\xspace}
\newcommand{\cop}{\textsc{COP}\xspace}
\newcommand{\cfn}{\textsc{CFN}\xspace}
\newcommand{\wcsp}{\textsc{WCSP}\xspace}
\newcommand{\ghost}{\textsc{GHOST}\xspace}

\newcommand{\flo}{\textcolor{blue}{\bf Flo}\xspace}
\newcommand{\jf}{\textcolor{red}{\bf JF}\xspace}

\begin{document}

\maketitle

\begin{abstract}
  A formidable  abstract that shows  off how much our  contribution is
  efficient and user-friendly. Oh,  and we have impressive experiments
  too. Be sure to subcribe our GitHub repository.
\end{abstract}

\section{Introduction}\label{sec:introduction}

TODO: CP  est difficile à  modéliser~\cite{Puget2004,Wallace2003}, CFN
ajoute une structure  utile au solveur mais est  également difficile à
modéliser  (même plus)  : générer  automatiquement les  CF pour  aider
l'utilisateur à modéliser son problème.  Pour palier à la modélisation
CP, on se repose sur  l'utilisation de méta-heuristiques qui n'ont pas
besoin       d'une        sélection       fine        des       bonnes
contraintes. \cite{AMJFH2011,Bessiere2015,CBLS}

Motivation : pas forcément facile ni intuitif de trouver une bonne CF.


\subsection{Of the interest of \cfn over classic CSP}
As shown in  [previous studies], using a cost function  to guide a CSP
solver, even for a satisfaction problem, generally greatly improve the
convergence speed to a solution.  It is important to note that using a
\cfn with  a manual input of  the cost functions (that  is the current
general usage of such solvers) greatly increase the difficulty for the
user.

The aim of  this research is to  provide a set of tools  to compute or
estimate those  cost functions  in a  way that  provides the  user the
following guarantees: CSP $<$ CFN-auto $\leq$ CFN-manual.

We hope  to eventually  provides methods that  can lead  to CFN-manual
$\leq$ CFN-auto for some (if not not most) families of constraints.

\section{Preliminaries}\label{sec:preliminaries}
TODO: Formalisation    WCSP     vs    CFN     (section    5     du
draft) \flo. \cite{Bessiere2011,LK2014}

Topo sur CPPN \jf \cite{CPPN}.

In the  literature, Cost  Function Networks  (\cfn) and  Weighted \csp
(\wcsp)  are  synonyms~\cite{Zytnicki2009,Bessiere2011}.  Some  papers
like~\cite{Allouche2012} present  \cfn to  be the formalism  and \wcsp
the  problem of  finding an  assignment minimizing  the combined  cost
function of a given \cfn instance. Since  it is rarely a good think in
Science to have two different names for the same notion, we start this
paper by proposing clear, distinct definitions of \cfn and \wcsp.

\subsection{Definitions of \wcsp and \cfn \flo} 

We propose  to keep the definition  of a \wcsp from  the literature: a
\textbf{\wcsp} is a tuple ($V$,$D$,$C$,$F$)  where $V$ is a finite set
of variables,  $D$ a finite set  of domains, one for  each variable in
$V$, each domain  being the set of  values a variable can  take, $C$ a
finite set  of constraints  over variables  in $V$,  determining which
combinations of values in $D$ are allowed or forbidden to describe our
problem, and finally a finite set  $F$ composed of cost functions, one
for  each constraint  in $C$.   Let's  denote by  $D_c$ the  Cartesian
product of  the domain of  variables involved  in a constraint  $c \in
C$. The cost function $f_c \in  F$ associated to the constraint $c$ is
a     function     $f_c:     D_c    \rightarrow     \{0,k\}$     where
$k \in \mathbb{N} \cup \{\infty\}$ is the special cost for assignments
violating the  constraint $c$. Thus, an  assignment $\alpha$ satisfies
the constraint  $c$ iff  $f_c(\alpha) < k$  holds. The  function $f_c$
allows  us to  rank assignments  and  to express  softness within  the
constraint $c$.

A \textbf{\cfn} is also defined  by a tuple ($V$,$D$,$C$,$F$) with the
same  sets  as   \wcsp.   The  difference  we  propose   lies  in  the
interpretation of cost functions $f_c$.  In this paper, cost functions
defined   in   a   \cfn    are   functions   $f_c:   D_c   \rightarrow
\mathbb{R}^+$. An assignment $\alpha$ satisfies the constraint $c$ iff
$f_c(\alpha) = 0$ holds. All  other strictly positive outputs of $f_c$
lead to forbidden assignments. Therefore, \cfn is considering hard (or
crisp) constraints  only, unlike  \wcsp. Strictly positive  outputs of
$f_c$ are then interpreted  like preferences over invalid assignments:
the closer $f_c(\alpha)$ is to 0, the closer $\alpha$ is to be a valid
assignment.

In the same  way a \csp instance  is a network of  constraints, \ie, a
networks of  predicates expressing if  an assignment satisfies  or not
each constraint, a \cfn instance  is a network of functions expressing
if  an assignment  satisfies the  constraints or  how close  it is  to
satisfy them. Thus,  this formalism \cfn allows us to  express a finer
structure about the  problem, since we furnish with  cost functions an
ordered  structure  over  invalid  assignment  a  solver  can  exploit
efficiently   to   improve  the   search.   We   illustrate  this   in
Section~\ref{sec:xp}.

Observe we  can also deal  with optimization  problems with a  \cfn by
adding  to  the  tuple  ($V$,$D$,$C$,$F$)  an  objective  function  to
optimize.

% \wcsp and \cfn are defined by close but different flavors throughout
% the litterature. To make the most  of both, we chose to consider the
% following definitions for those two concepts.

%  \paragraph{\wcsp}   is  considering   soft  constraints,   where  a
% constraint is unsatisfyed iif its associated cost function outputs a
% value below a given threshold $k$ (can be infinite).

%  \paragraph{\cfn}  is  considering   hard  constraints  only.   Like
% constraint  networks helping solvers  to find solutions by  giving a
% structure of  the problem, CFN gives, in addition  of the constraint
%  network,  a structure  on  configurations  to  help the  solver  to
% determine if an unsatisfying configuration  is near to be a solution
% or not.

From those  definition, we  can consider \wcsp  and the  \cfn variants
(satisfaction  and  optimization)  to have  different  expressiveness:
\cfn-sat $<$ \wcsp $<$ \cfn-opt

In  this study,  we deliberately  chose  to focus  on \cfn  as we  can
naturally transform any \wcsp into \cfn-opt.

\section{Theoretical Design \jf}\label{sec:theory}
From  head to  toes,  how our  framework  is designed  (interpolation,
pre-processing and such) to fit a solver (here GHOST).

Modèles (qui n'ont pas marché) :
\begin{itemize}
\item CF comme combinaison linéaire de sinus
\item CF en CPPN de fonctions simples (sinus, tanh, sigmoid, ...)
\item CF apprise par CFN (lol) avec smoothness comme fonction objectif.
\item Relaxation du SL en regardant seulement l'ordre de Hamming avec le modèle CPPN.
\end{itemize}

\subsection{Scaling Issues}\label{subsec:issues}
Scaling  our  results  on  small instances  is  challenging.  In  this
subsection we cover 4 possible approaches that are either unsuccessful
or not yet efficient.

\paragraph{Learning a cost function on small space}
For general constraints, there is no generic idea to extend a known cost
function from a small space to a higher dimension one.

\section{Experimental Results}\label{sec:xp}
Presenting the aim  of those experiments, the choice  of the datasets,
and the results itself.

Type des  contraintes abordées.  Pour l'instant  : Comparison:allDiff,
Comparison:ordered\_lessThan, Counting:linear\_sum, Connection:min:GT,
Scheduling:noOverlap1D.

Apprendre une  CF sur une petit  isntance, et la comparer  avec 1. les
métriques  sur cette  même instance,  2.  les métriques  sur une  plus
grande instance et 3. Une CF apprise sur une plus grande instance.

Comparaison des CF obtenues avec différent pourcentage de l'espace des
configurations samplée.

Comparaison des  CF obtenues sur des  benchs avec des CF  handmades et
sans CF

Généralisation  : apprendre  une CF  pour une  instance avec  certains
paramètres  et  tester sur  d'autres  paramètres  (et/ou sur  d'autres
tailles aussi)

Présentation et analyse \flo.
Figure et tableaux \jf.

\subsection{Automatic Generation of Cost Functions}
\label{subsec:xpgeneration}

\subsection{Comparison with crafted cost functions}\label{subsec:xpcomparison}

\section{Discussions}

Avantage  de  notre  modèle  :  les  CF  trouvées  sont  intelligibles
(contrairement aux NN). On peut même les coder en dur si l'on préfère,
plutôt que de calculer la CF avec un run forward du CPPN.

Peut être utilisé pour de l'aide à la décision, cad aider à définir un
CF à la main.

Autre avantage  : les CF  apprises sont robustes car  indépendantes de
l'instance des contraintes (taille et valeurs des paramètres)

La  simplification CPPN  où  les  poids sont  booléens  est aussi  une
avantage  pour  simplifier  la représentation  (et  compréhension,  et
reproduction) de la CF par rapport à des poids dans [0,1].

CPPN facile  à modifier (ses  opérations, notamment) pour  des besoins
spécifiques, des problèmes ou des contraintes particulières.


\section{Conclusion and perspectives}\label{sec:conclusion}

Faire du  RL plutôt  que du  SL. Pas  sûr que  Hamming soit  une bonne
métrique (voir  linear\_eq). Cela  permettra de  trouver des  CF mieux
adaptées aux algos.


\bibliographystyle{named}
\bibliography{main}

\end{document}
