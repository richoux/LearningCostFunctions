\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai20.sty is NOT the same than previous years'
\usepackage{ijcai20}

\usepackage[english]{babel}
\usepackage[T1]{fontenc}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}

\usepackage{xspace}
\usepackage[dvipsnames, table]{xcolor}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{adjustbox}
%\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = blue %Colour of citations
}

%\title{Automatic Cost Function Generation ('learning' in the title?)}
\title{Automatic Learning of Cost Functions to Help Modeling Cost Function Networks}

\iffalse
\author{
Florian Richoux$^{1,2}$
\and
Jean-François Baffier$^3$
\affiliations
$^1$JFLI, CNRS, NII\\
$^2$Université de Nantes\\
$^3$RIKEN AIP
\emails
florian.richoux@polytechnique.edu,
jf@baffier.fr
}
\fi

\newcommand{\ie}{\textit{i.e.}}
\newcommand{\cp}{\textsc{CP}\xspace}
\newcommand{\csp}{\textsc{CSP}\xspace}
\newcommand{\cop}{\textsc{COP}\xspace}
\newcommand{\cfn}{\textsc{CFN}\xspace}
\newcommand{\cf}{\textsc{cf}\xspace}
\newcommand{\wcsp}{\textsc{WCSP}\xspace}
\newcommand{\ghost}{\textsc{GHOST}\xspace}

\newcommand{\flo}{\textcolor{blue}{\bf Flo}\xspace}
\newcommand{\jf}{\textcolor{red}{\bf JF}\xspace}

\begin{document}

\maketitle

\begin{abstract}
  Cost  Function  Networks  (\cfn)   are  a  formalism  in  Constraint
  Programming  to  model  combinatorial satisfaction  or  optimization
  problems.   By associating  a function  to each  constraint type  to
  evaluate the quality of an assignment, we extend the expressivity of
  regular \csp/\cop  formalisms but  at a price  of making  harder the
  problem    modeling.     Indeed,     in    addition    to    regular
  variables/domains/constraints sets,  one must provide a  set of cost
  functions  (\cf) that  are  not  always easy  to  define.  Here  we
  propose a method to automatically learn a \cf of a constraint, given
  a function deciding if assignments are valid or not.  This is to the
  best  of our  knowledge  the first  attempt  to automatically  learn
  $\cf$s.  Our  method aims to  learn $\cf$s in a  supervised fashion,
  trying to  reproduce the Hamming  distance, by using a  variation of
  neural  networks  allowing us  to  get  explainable results,  unlike
  regular artificial neural networks.  We experiment it on 5 different
  constraints  to   show  its  versatility.   Experiments   show  that
  functions  learned on  small  dimensions scale  on high  dimensions,
  outputting a  perfect or near-perfect Hamming  distance.  Our method
  can be  used to  automatically generate $\cf$s  and then  having the
  expressivity  of  \cfn  with  the  same  modeling  effort  than  for
  \csp/\cop.
\end{abstract}

\section{Introduction}\label{sec:introduction}

TODO: CP  est difficile à  modéliser~\cite{Puget2004,Wallace2003}, CFN
ajoute une structure  utile au solveur mais est  également difficile à
modéliser  (même plus)  : générer  automatiquement les  CF pour  aider
l'utilisateur à modéliser son problème.  Pour palier à la modélisation
CP, on se repose sur  l'utilisation de méta-heuristiques qui n'ont pas
besoin       d'une        sélection       fine        des       bonnes
contraintes. \cite{AMJFH2011,Bessiere2015,CBLS}

Motivation : pas forcément facile ni intuitif de trouver une bonne CF.


\subsection{Of the interest of \cfn over classic CSP}
As shown in  [previous studies], using a cost function  to guide a CSP
solver, even for a satisfaction problem, generally greatly improve the
convergence speed to a solution.  It is important to note that using a
\cfn with  a manual input of  the cost functions (that  is the current
general usage of such solvers) greatly increase the difficulty for the
user.

The aim of  this research is to  provide a set of tools  to compute or
estimate those  cost functions  in a  way that  provides the  user the
following guarantees: CSP $<$ CFN-auto $\leq$ CFN-manual.

We hope  to eventually  provides methods that  can lead  to CFN-manual
$\leq$ CFN-auto for some (if not not most) families of constraints.

\section{Preliminaries}\label{sec:preliminaries}
TODO: Formalisation    WCSP     vs    CFN     (section    5     du
draft) \flo. \cite{Bessiere2011,LK2014}

Topo sur CPPN \jf \cite{CPPN}.

In the  literature, Cost  Function Networks  (\cfn) and  Weighted \csp
(\wcsp)  are  synonyms~\cite{Zytnicki2009,Bessiere2011}.  Some  papers
like~\cite{Allouche2012} present  \cfn to  be the formalism  and \wcsp
the  problem of  finding an  assignment minimizing  the combined  cost
function of a given \cfn instance. Since  it is rarely a good think in
Science to have two different names for the same notion, we start this
paper by proposing clear, distinct definitions of \cfn and \wcsp.

\subsection{Definitions of \wcsp and \cfn \flo} 

We propose  to keep the definition  of a \wcsp from  the literature: a
\textbf{\wcsp} is a tuple ($V$,$D$,$C$,$F$)  where $V$ is a finite set
of variables,  $D$ a finite set  of domains, one for  each variable in
$V$, each domain  being the set of  values a variable can  take, $C$ a
finite set  of constraints  over variables  in $V$,  determining which
combinations of values in $D$ are allowed or forbidden to describe our
problem, and finally a finite set  $F$ composed of cost functions, one
for  each constraint  in $C$.   Let's  denote by  $D_c$ the  Cartesian
product of  the domain of  variables involved  in a constraint  $c \in
C$. The cost function $f_c \in  F$ associated to the constraint $c$ is
a     function     $f_c:     D_c    \rightarrow     \{0,k\}$     where
$k \in \mathbb{N} \cup \{\infty\}$ is the special cost for assignments
violating the  constraint $c$. Thus, an  assignment $\alpha$ satisfies
the constraint  $c$ iff  $f_c(\alpha) < k$  holds. The  function $f_c$
allows  us to  rank assignments  and  to express  softness within  the
constraint $c$.

A \textbf{\cfn} is also defined  by a tuple ($V$,$D$,$C$,$F$) with the
same  sets  as   \wcsp.   The  difference  we  propose   lies  in  the
interpretation of cost functions $f_c$.  In this paper, cost functions
defined   in   a   \cfn    are   functions   $f_c:   D_c   \rightarrow
\mathbb{R}^+$. An assignment $\alpha$ satisfies the constraint $c$ iff
$f_c(\alpha) = 0$ holds. All  other strictly positive outputs of $f_c$
lead to forbidden assignments. Therefore, \cfn is considering hard (or
crisp) constraints  only, unlike  \wcsp. Strictly positive  outputs of
$f_c$ are then interpreted  like preferences over invalid assignments:
the closer $f_c(\alpha)$ is to 0, the closer $\alpha$ is to be a valid
assignment.

In the same  way a \csp instance  is a network of  constraints, \ie, a
networks of  predicates expressing if  an assignment satisfies  or not
each constraint, a \cfn instance  is a network of functions expressing
if  an assignment  satisfies the  constraints or  how close  it is  to
satisfy them. Thus,  this formalism \cfn allows us to  express a finer
structure about the  problem, since we furnish with  cost functions an
ordered  structure  over  invalid  assignment  a  solver  can  exploit
efficiently   to   improve  the   search.   We   illustrate  this   in
Section~\ref{sec:xp}.

Observe we  can also deal  with optimization  problems with a  \cfn by
adding  to  the  tuple  ($V$,$D$,$C$,$F$)  an  objective  function  to
optimize.

% \wcsp and \cfn are defined by close but different flavors throughout
% the litterature. To make the most  of both, we chose to consider the
% following definitions for those two concepts.

%  \paragraph{\wcsp}   is  considering   soft  constraints,   where  a
% constraint is unsatisfyed iif its associated cost function outputs a
% value below a given threshold $k$ (can be infinite).

%  \paragraph{\cfn}  is  considering   hard  constraints  only.   Like
% constraint  networks helping solvers  to find solutions by  giving a
% structure of  the problem, CFN gives, in addition  of the constraint
%  network,  a structure  on  configurations  to  help the  solver  to
% determine if an unsatisfying configuration  is near to be a solution
% or not.

From those  definition, we  can consider \wcsp  and the  \cfn variants
(satisfaction  and  optimization)  to have  different  expressiveness:
\cfn-sat $<$ \wcsp $<$ \cfn-opt

In  this study,  we deliberately  chose  to focus  on \cfn  as we  can
naturally transform any \wcsp into \cfn-opt.

\section{Theoretical Design \jf}\label{sec:theory}
From  head to  toes,  how our  framework  is designed  (interpolation,
pre-processing and such) to fit a solver (here GHOST).

Modèles (qui n'ont pas marché) :
\begin{itemize}
\item CF comme combinaison linéaire de sinus
\item CF en CPPN de fonctions simples (sinus, tanh, sigmoid, ...)
\item CF apprise par CFN (lol) avec smoothness comme fonction objectif.
\item Relaxation du SL en regardant seulement l'ordre de Hamming avec le modèle CPPN.
\end{itemize}

\subsection{Scaling Issues}\label{subsec:issues}
Scaling  our  results  on  small instances  is  challenging.  In  this
subsection we cover 4 possible approaches that are either unsuccessful
or not yet efficient.

\paragraph{Learning a cost function on small space}
For general constraints, there is no generic idea to extend a known cost
function from a small space to a higher dimension one.

\section{Experiments}\label{sec:xp}

To  show  the versatility  of  our  method, we  tested  it  on 5  very
different constraints:  AllDifferent, Ordered,  LinearSum, NoOverlap1D
and                Minimum.                According                to
\href{http://xcsp.org/specifications}{xcsp.org/specifications},  those
global  constraints   below  to   4  different   families:  Comparison
(AllDifferent    and     Ordered),    Counting/Summing    (LinearSum),
Packing/Scheduling  (NoOverlap1D)  and Connection  (Minimum).   Always
according to XCSP specifications, these 5 constraints are among the 20
most popular  and common constraints.  We give a brief  description of
those 5 constraints below:

\paragraph{AllDifferent} ensures  that variables must all  be assigned
to different values.
\paragraph{Ordered} ensures  that an  assignment of variables  must be
ordered, given a total order. In this paper, we choose the total order
$\leq$. Thus, for all indexes $i,j \in \{1,n\}$ with $n$ the number of
variables, $i < j$ implies $x_i \leq x_j$.
\paragraph{LinearSum}       ensures       that      the       equation
$x_1 + x_2 +  \ldots + x_n = p$ holds, with the  parameter $p$ a given
integer.
\paragraph{NoOverlap1D}  is considering  variables as  tasks, starting
from a  certain time (their  value) and each  with a given  length $p$
(their  parameter).    The  constraint  ensures  that   no  tasks  are
overlapping,  \ie, for  all indexes  $i,j  \in \{1,n\}$  with $n$  the
number   of   variables,  we   have   $x_i   +   p_i  \leq   x_j$   or
$x_j + p_j \leq  x_i$.  To have a simpler code,  we have considered in
this paper that all tasks have the same length $p$.
\paragraph{Minimum} ensures  that the  minimum value of  an assignment
verifies  a given  numerical condition.  In this  paper, we  choose to
consider that  the minimum value must  be greater than or  equals to a
given parameter $p$.

\subsection{Experimental protocol}



Apprendre une  CF sur une petit  instance, et la comparer  avec 1. les
métriques  sur cette  même instance,  2.  les métriques  sur une  plus
grande instance et 3. Une CF apprise sur une plus grande instance.

Comparaison des CF obtenues avec différent pourcentage de l'espace des
configurations samplée.

Comparaison des  CF obtenues sur des  benchs avec des CF  handmades et
sans CF

Généralisation  : apprendre  une CF  pour une  instance avec  certains
paramètres  et  tester sur  d'autres  paramètres  (et/ou sur  d'autres
tailles aussi)

Présentation et analyse \flo.
Figure et tableaux \jf.

\subsection{Results}

\subsection{Automatic Generation of Cost Functions}
\label{subsec:xpgeneration}

\subsection{Comparison with crafted cost functions}\label{subsec:xpcomparison}

\section{Discussions}

Résultats améliorables  (notamment en  terme de  stabilité d'obtention
d'une CF optimale) en optimisant le GA, ce que nous n'avons pas fait.

Avantage  de  notre  modèle  :  les  CF  trouvées  sont  intelligibles
(contrairement aux NN). On peut même les coder en dur si l'on préfère,
plutôt que de calculer la CF avec un run forward du CPPN.

Peut être utilisé pour de l'aide à la décision, cad aider à définir un
CF à la main.

Autre avantage  : les CF  apprises sont robustes car  indépendantes de
l'instance des contraintes (taille et valeurs des paramètres)

La  simplification CPPN  où  les  poids sont  booléens  est aussi  une
avantage  pour  simplifier  la représentation  (et  compréhension,  et
reproduction) de la CF par rapport à des poids dans [0,1].

CPPN facile  à modifier (ses  opérations, notamment) pour  des besoins
spécifiques, des problèmes ou des contraintes particulières.


\section{Conclusion and perspectives}\label{sec:conclusion}

Faire du  RL plutôt  que du  SL. Pas  sûr que  Hamming soit  une bonne
métrique. Cela permettra de trouver des CF mieux adaptées aux algos.


\bibliographystyle{named}
\bibliography{main}

\end{document}
